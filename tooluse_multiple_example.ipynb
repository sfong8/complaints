{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2c7c1cd-c9a1-4ee3-813a-c7433d64efc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, json, math\n",
    "\n",
    "class ToolError(Exception):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9568b53b-1052-476d-a256-6dba65aa25b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md\t\t  complaints_tooluse_cleaned.ipynb\n",
      "complaints_tooluse.ipynb  tooluse_multiple_example-Copy1.ipynb\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b59c1112-c7dc-4c16-bc91-9677b0c871f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5173/706041567.py:15: LangChainDeprecationWarning: The class `BedrockEmbeddings` was deprecated in LangChain 0.2.11 and will be removed in 1.0. An updated version of the class exists in the langchain-aws package and should be used instead. To use it run `pip install -U langchain-aws` and import as `from langchain_aws import BedrockEmbeddings`.\n",
      "  embeddings = BedrockEmbeddings(\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'complaints_fake.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5173/706041567.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocuments\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDocument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mdocuments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'complaints_fake.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.11/site-packages/langchain_community/vectorstores/faiss.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(cls, folder_path, embeddings, index_name, allow_dangerous_deserialization, **kwargs)\u001b[0m\n\u001b[1;32m   1201\u001b[0m         \u001b[0;31m# load index separately since it is not picklable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m         \u001b[0mfaiss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdependable_faiss_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1203\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfaiss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34mf\"{index_name}.faiss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.11/site-packages/faiss/swigfaiss.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m  10537\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 10538\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_swigfaiss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Error in faiss::FileIOReader::FileIOReader(const char*) at /home/conda/feedstock_root/build_artifacts/faiss-split_1723208843907/work/faiss/impl/io.cpp:67: Error: 'f' failed: could not open complaints.vs/index.faiss for reading: No such file or directory",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocuments\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Document\n\u001b[1;32m     28\u001b[0m documents \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 29\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcomplaints_fake.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m     31\u001b[0m     document_temp \u001b[38;5;241m=\u001b[39m Document(page_content\u001b[38;5;241m=\u001b[39mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomplaint_text\u001b[39m\u001b[38;5;124m'\u001b[39m],metadata\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclient_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclient_name\u001b[39m\u001b[38;5;124m'\u001b[39m],\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclient_region\u001b[39m\u001b[38;5;124m\"\u001b[39m: row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclient_region\u001b[39m\u001b[38;5;124m'\u001b[39m],\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtheme\u001b[39m\u001b[38;5;124m\"\u001b[39m: row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtheme\u001b[39m\u001b[38;5;124m'\u001b[39m],\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomplaint_date\u001b[39m\u001b[38;5;124m\"\u001b[39m: row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomplaint_date\u001b[39m\u001b[38;5;124m'\u001b[39m]})\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'complaints_fake.csv'"
     ]
    }
   ],
   "source": [
    "import boto3, json, math\n",
    "import faiss\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize the Bedrock runtime client\n",
    "bedrock_client = boto3.client(\n",
    "    service_name=\"bedrock-runtime\",\n",
    "    region_name='us-east-1',\n",
    ")\n",
    "\n",
    "# Create an instance of BedrockEmbeddings using the Bedrock client\n",
    "embeddings = BedrockEmbeddings(\n",
    "    client=bedrock_client, \n",
    "    model_id=\"amazon.titan-embed-text-v2:0\"\n",
    ")\n",
    "try:\n",
    "# Load the previously created FAISS vector store for complaints\n",
    "    vector_store = FAISS.load_local('complaints.vs', embeddings, allow_dangerous_deserialization=True)\n",
    "except:\n",
    "    import faiss\n",
    "    from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "    from langchain_community.vectorstores import FAISS\n",
    "    from uuid import uuid4\n",
    "    from langchain_core.documents import Document\n",
    "    documents = []\n",
    "    df = pd.read_csv(r'complaints_fake.csv')\n",
    "    for index, row in df.iterrows():\n",
    "        document_temp = Document(page_content=row['complaint_text'],metadata={\"client_name\": row['client_name'],\"client_region\": row['client_region'],\"theme\": row['theme'],\"complaint_date\": row['complaint_date']})\n",
    "        documents.append(document_temp)\n",
    "    vector_store = FAISS.from_documents(documents=documents,embedding=embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92617d20-a137-4dea-8336-d5c842677bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getContext(user_prompt:str,filter_terms:dict):\n",
    "    \"\"\"\n",
    "    This function retrieves the context (complaints) based on the user's query.\n",
    "\n",
    "    Parameters:\n",
    "    user_prompt (str): The user's query.\n",
    "    vector_store: The FAISS vector store containing the complaints.\n",
    "    tool_list (list): The list of tools to use for filtering.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame containing the retrieved complaints.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract the metadata filter and the number of documents to return from the filter response\n",
    "        metadata_filter = filter_terms['metadata_filter']\n",
    "        k_filter = filter_terms['k_filter']\n",
    "        print(f\"filter by metadata: {metadata_filter} and k: {k_filter}\")\n",
    "        \n",
    "        # Create a retriever with the extracted filters\n",
    "        retriever = vector_store.as_retriever(search_kwargs={'filter': metadata_filter, 'k':k_filter})\n",
    "    except:\n",
    "        print('cannot extract out filters so going default')\n",
    "        # Create a retriever with default settings if filters cannot be extracted\n",
    "        retriever = vector_store.as_retriever(search_kwargs= {'k':100})\n",
    "\n",
    "    # Retrieve the documents using the retriever\n",
    "    docs = retriever.invoke(user_prompt)\n",
    "    \n",
    "    # Create a DataFrame to store the retrieved complaints\n",
    "    master_df = pd.DataFrame()\n",
    "    for doc in docs:\n",
    "        test=doc.metadata\n",
    "        test.update({'complaint_text':doc.page_content})\n",
    "        temp_df = pd.DataFrame([test.values()],columns=test.keys())\n",
    "        master_df = pd.concat([master_df,temp_df])\n",
    "    \n",
    "    return master_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc2e0b2-cb4f-4dfa-84e7-02b4523220c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "# Initialize the Bedrock runtime client\n",
    "bedrock_client = boto3.client(\n",
    "    service_name=\"bedrock-runtime\",\n",
    "    region_name='us-east-1',\n",
    ")\n",
    "\n",
    "def call_bedrock(message_list, system_prompts, tool_list):\n",
    "    \"\"\"\n",
    "    This function interacts with Amazon Bedrock Converse API to generate responses using a specified model and tools.\n",
    "\n",
    "    Parameters:\n",
    "    message_list (list): A list of messages to send to the model.\n",
    "    system_prompts (str): System prompts to guide the model's response.\n",
    "    tool_list (list): A list of tools to be used by the model.\n",
    "\n",
    "    Returns:\n",
    "    dict: The response from the Bedrock service.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Call the Bedrock converse API with tool configuration\n",
    "    response = bedrock_client.converse(\n",
    "        modelId=\"amazon.nova-pro-v1:0\",  # ID of the model to use\n",
    "        messages=message_list,  # Messages to send to the model\n",
    "        system=[{ 'text': system_prompts }],  # System prompts\n",
    "        inferenceConfig={  # Inference configuration\n",
    "            \"maxTokens\": 2000,  # Maximum number of tokens to generate\n",
    "            \"temperature\": 0.1  # Temperature for response randomness\n",
    "        },\n",
    "        toolConfig={ \"tools\": tool_list }  # Tool configuration\n",
    "    )\n",
    "    \n",
    "    # Return the response from the Bedrock service\n",
    "    return response\n",
    "\n",
    "def process_tool_uses(message_list, system_prompts, tool_list):\n",
    "    \"\"\"\n",
    "    This function processes tool uses iteratively until a final response is generated.\n",
    "\n",
    "    Parameters:\n",
    "    message_list (list): A list of messages to send to the model.\n",
    "    system_prompts (str): System prompts to guide the model's response.\n",
    "    tool_list (list): A list of tools to be used by the model.\n",
    "\n",
    "    Returns:\n",
    "    str: The final generated response.\n",
    "    \"\"\"\n",
    "    \n",
    "    while True:\n",
    "        response = call_bedrock(message_list, system_prompts, tool_list)\n",
    "        print(response)\n",
    "        for content in response['output']['message']['content']:\n",
    "            if 'toolUse' in content:\n",
    "                tool_use = content['toolUse']\n",
    "                \n",
    "                tool_name = tool_use['name']\n",
    "                tool_input = tool_use['input']\n",
    "                \n",
    "                if tool_name == \"identify_complaints_filters\":\n",
    "                    # Simulate identifying filters based on the query\n",
    "                    filters = tool_input\n",
    "                    print(f\"--------------------------------\\n Identified filters: {filters}\")\n",
    "                    \n",
    "                    # Update the tool list to call the next tool with the identified filters\n",
    "                    tool_list = [\n",
    "                        {\n",
    "                            \"toolSpec\": {\n",
    "                                \"name\": \"get_complaints_data\",\n",
    "                                \"input\": {\n",
    "                                    \"filters\": filters\n",
    "                                }\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                    break  # Break to re-call the API with the updated tool list\n",
    "                \n",
    "            \n",
    "            else:\n",
    "                # Extract the final response if no tool uses are present\n",
    "                final_response = response['output']['message']['content'][0]['text']\n",
    "                print(f\"Generated response: {final_response}\")\n",
    "                return final_response\n",
    "\n",
    "# Define the tool list\n",
    "tool_list =[\n",
    "    {\n",
    "        \"toolSpec\": {\n",
    "            \"name\": \"identify_complaints_filters\",\n",
    "            \"description\": \"\"\"\"Your job is to first look at the query's query and determine if it requires filtering the complaints first based off the following columns \n",
    "            client_name - name of the company making the complaint, you will need to use the like filter since users might not type if the exact name\n",
    "            client_region - possible values ['MC','LC','ICB'] where MC= mid-corporate, LC = Large corporate, ICB = International corprorate\n",
    "            complaint_date - date of the complaint\n",
    "\n",
    "            Note: if you're not sure then don't output anything. This is used for metadata filter for the vectorstore. There are two outputs x and y, where x is the column filters and y is the number of documents to return - if user doesnt specify then default is 100\n",
    "\n",
    "            Example1: user_query: Show me all complaints in region MC\n",
    "            output: = {'metadata_filter': {client_region\": \"MC\"},'k_filter':100}\n",
    "\n",
    "            Example2: user_query: Show me all complaints in region LC and after 15th June 2024\n",
    "            output: {'metadata_filter':{\"client_region\": \"MC\",\"complaint_date\": \">2024-06-15\"},'k_filter':100}\n",
    "\n",
    "\n",
    "            Example3: user_query: Show me 5 complaints in regions MC and LC\n",
    "            output: {'metadata_filter':{client_region\": [\"MC\",'LC']},'k_filter':5}  \"\"\",\n",
    "            \"inputSchema\": {\n",
    "                \"json\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"filter_dict\": {\n",
    "                            \"type\": \"dict\",\n",
    "                            \"description\": \"\"\"output filter as a dict e.g. {'metadata_filter':{client_region\": [\"MC\",'LC']},'k_filter':5} \"\"\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"filter_dict\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"toolSpec\": {\n",
    "            \"name\": \"get_complaintsData\",\n",
    "            \"description\": \"\"\"This function retrieves the context (complaints) based on the user's query.\"\"\",\n",
    "            \"inputSchema\": {\n",
    "                \"json\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"user_query\": {\n",
    "                            \"type\": \"str\",\n",
    "                            \"description\": \"\"\"original user query\"\"\"\n",
    "                        },\n",
    "                         \"filter_terms\": {\n",
    "                            \"type\": \"dict\",\n",
    "                            \"description\": \"\"\"filter term from the output of tool identify_complaints_filters\"\"\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"user_query\",\"filter_terms\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Define the message list and system prompts\n",
    "message_list = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [ { \"text\": \"Show me all complaints in region MC after 15th June 2024\" } ]\n",
    "    }\n",
    "]\n",
    "\n",
    "system_prompts = \"\"\"You are an AI assistant within a corporate bank in the Complaints team. \n",
    "Your role is to retrieve and analyze complaints based on user queries. \n",
    "Use the provided tools to filter, retrieve, and generate responses.\"\"\"\n",
    "\n",
    "# # Process tool uses iteratively\n",
    "# final_response = process_tool_uses(message_list, system_prompts, tool_list)\n",
    "# print(f\"Final response: {final_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f498e769-cb3e-4c34-a6b4-344b04f6a680",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tool_result(tool_use_block):\n",
    "    \n",
    "    tool_use_name = tool_use_block['name']\n",
    "            \n",
    "    print(f\"Using tool {tool_use_name}\")\n",
    "    \n",
    "    # Note: We're deliberately excluding tangent so something magical can happen\n",
    "    if tool_use_name == 'identify_complaints_filters':\n",
    "        return math.cos(tool_use_block['input']['x'])\n",
    "    elif tool_use_name == 'sine':\n",
    "        return math.sin(tool_use_block['input']['x'])\n",
    "    elif tool_use_name == 'divide_numbers':\n",
    "        return tool_use_block['input']['x'] / tool_use_block['input']['y'] \n",
    "    else:\n",
    "        raise ToolError(f\"Invalid function name: {tool_use_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791631ac-488a-4d5d-afa6-713dbf5c104b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = call_bedrock(message_list, system_prompts, tool_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba83acce-d791-40bd-b6bc-357927285e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bff0b13-eef8-4e29-ad16-f456c24e9e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for content in response['output']['message']['content']:\n",
    "    if 'toolUse' in content:\n",
    "        tool_use = content['toolUse']\n",
    "        \n",
    "        tool_name = tool_use['name']\n",
    "        tool_input = tool_use['input']\n",
    "        \n",
    "        if tool_use['name']=='identify_complaints_filters':\n",
    "            try:\n",
    "                    follow_up_content_blocks.append({\n",
    "                        \"toolResult\": {\n",
    "                            \"toolUseId\": tool_use_block['toolUseId'],\n",
    "                            \"toolUseName\": \"identify_complaints_filters\",\n",
    "                            \"content\": [\n",
    "                                { \"json\": { \"filter_terms\": tool_use_block['input'] } }\n",
    "                            ]\n",
    "                        }\n",
    "                    })\n",
    "                \n",
    "            except ToolError as e:\n",
    "                follow_up_content_blocks.append({ \n",
    "                    \"toolResult\": {\n",
    "                        \"toolUseId\": tool_use_block['toolUseId'],\n",
    "                        \"toolUseName\": \"identify_complaints_filters\",\n",
    "                        \"content\": [  { \"text\": repr(e) } ],\n",
    "                        \"status\": \"error\"\n",
    "                    }\n",
    "                })\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8aeca9-e3e3-4263-a67a-c2b9af65909b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tool_result(tool_use_block):\n",
    "    \n",
    "    tool_use_name = tool_use_block['name']\n",
    "            \n",
    "    print(f\"Using tool {tool_use_name}\")\n",
    "    \n",
    "    # Note: We're deliberately excluding tangent so something magical can happen\n",
    "    if tool_use_name == 'cosine':\n",
    "        return math.cos(tool_use_block['input']['x'])\n",
    "    elif tool_use_name == 'sine':\n",
    "        return math.sin(tool_use_block['input']['x'])\n",
    "    elif tool_use_name == 'divide_numbers':\n",
    "        return tool_use_block['input']['x'] / tool_use_block['input']['y'] \n",
    "    else:\n",
    "        raise ToolError(f\"Invalid function name: {tool_use_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5369d2ca-d411-40e8-8e30-5c7b8b2228d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_response(response_message):\n",
    "    \n",
    "    response_content_blocks = response_message['content']\n",
    "    \n",
    "    follow_up_content_blocks = []\n",
    "    \n",
    "    for content_block in response_content_blocks:\n",
    "        if 'toolUse' in content_block:\n",
    "            tool_use_block = content_block['toolUse']\n",
    "\n",
    "            if tool_use_block['name']=='identify_complaints_filters':\n",
    "                try:\n",
    "                        follow_up_content_blocks.append({\n",
    "                            \"toolResult\": {\n",
    "                                \"toolUseId\": tool_use_block['toolUseId'],\n",
    "                                \"content\": [\n",
    "                                    { \"json\": { \"filter_terms\": tool_use_block['input'] } }\n",
    "                                ]\n",
    "                            }\n",
    "                        })\n",
    "                    \n",
    "                except ToolError as e:\n",
    "                    follow_up_content_blocks.append({ \n",
    "                        \"toolResult\": {\n",
    "                            \"toolUseId\": tool_use_block['toolUseId'],\n",
    "                            \"content\": [  { \"text\": repr(e) } ],\n",
    "                            \"status\": \"error\"\n",
    "                        }\n",
    "                    })\n",
    "            if tool_use_block['name']=='get_complaintsData':\n",
    "                user_query =  tool_use_block['input']['user_query']\n",
    "                filter_terms = tool_use_block['input']['filter_terms']\n",
    "                \n",
    "    \n",
    "    if len(follow_up_content_blocks) > 0:\n",
    "        \n",
    "        follow_up_message = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": follow_up_content_blocks,\n",
    "        }\n",
    "        \n",
    "        return follow_up_message\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7339dcb-9658-4c9d-a28e-7e2d83d41353",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_loop(prompt,system_prompts, tool_list):\n",
    "    MAX_LOOPS = 6\n",
    "    loop_count = 0\n",
    "    continue_loop = True\n",
    "    \n",
    "    message_list = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [ { \"text\": prompt } ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    while continue_loop:\n",
    "        response = call_bedrock(message_list,system_prompts, tool_list)\n",
    "        \n",
    "        response_message = response['output']['message']\n",
    "        message_list.append(response_message)\n",
    "        \n",
    "        loop_count = loop_count + 1\n",
    "        \n",
    "        if loop_count >= MAX_LOOPS:\n",
    "            print(f\"Hit loop limit: {loop_count}\")\n",
    "            break\n",
    "        \n",
    "        follow_up_message = handle_response(response_message)\n",
    "        \n",
    "        if follow_up_message is None:\n",
    "            # No remaining work to do, return final response to user\n",
    "            continue_loop = False \n",
    "        else:\n",
    "            message_list.append(follow_up_message)\n",
    "            \n",
    "    return message_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba29555e-cbf1-4035-9692-841f77c695b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "message_list = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [ { \"text\": \"Show me all complaints in region MC after 15th June 2024\" } ]\n",
    "    }\n",
    "]\n",
    "\n",
    "system_prompts = \"\"\"You are an AI assistant within a corporate bank in the Complaints team. \n",
    "Your role is to retrieve and analyze complaints based on user queries. \n",
    "Use the provided tools to filter, retrieve, and generate responses.\"\"\"\n",
    "\n",
    "\n",
    "messages = run_loop(\"Show me all complaints in region MC after 15th June 2024\",system_prompts, tool_list)\n",
    "\n",
    "print(\"\\nMESSAGES:\\n\")\n",
    "print(json.dumps(messages, indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6209472c-3d9d-4dd7-b84f-5c529b9c4727",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
